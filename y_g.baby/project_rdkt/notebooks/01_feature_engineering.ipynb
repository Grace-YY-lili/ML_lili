{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-25T09:17:54.511329Z",
     "iopub.status.busy": "2026-01-25T09:17:54.508307Z",
     "iopub.status.idle": "2026-01-25T09:17:56.012240Z",
     "shell.execute_reply": "2026-01-25T09:17:56.012240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¯åŠ¨ RDKit ç‰¹å¾å·¥ç¨‹ (Robust Mode) | ç›®å½•: D:\\HO\\my_smiles_project\\y_g.baby\\project_rdkt\n",
      "\n",
      ">>> [Step 1] Loading Raw Data...\n",
      "   è¯»å–æ–‡ä»¶: cdft_qsar.CSV | åŸå§‹è¡Œæ•°: 151\n",
      "   Target: 'log(1o2)' | SMILES: 'SMILES'\n",
      "\n",
      ">>> [Step 2] RDKit Processing (Canonicalization & Aggregation)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - åŸå§‹æœ‰æ•ˆåˆ†å­æ•°: 151\n",
      "   - æ— æ•ˆ/è§£æå¤±è´¥ SMILES: 0\n",
      "   - èšåˆåå”¯ä¸€ç»“æ„æ•°: 151 (åˆå¹¶äº† 0 ä¸ªé‡å¤ç»“æ„)\n",
      "\n",
      ">>> [Step 3] Stratified Splitting...\n",
      "   Seed: 288\n",
      "   Train: (105, 217), Val: (23, 217), Test: (23, 217)\n",
      "\n",
      ">>> [Step 4] Rigorous Cleaning (Fit on Train)...\n",
      "   -> Imputing...\n",
      "   -> Removing Constants...\n",
      "      Deleted 40 constant features\n",
      "   -> Removing High Correlation (>0.90)...\n",
      "      Deleted 51 correlated features\n",
      "   âœ… æœ€ç»ˆç‰¹å¾æ•°: 126\n",
      "\n",
      "ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³ data/processed/\n",
      "ğŸ“ ç‰¹å¾æ—¥å¿—å·²ä¿å­˜è‡³ results/logs/feature_log.json\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# File: project_rdkt/notebooks/01_feature_engineering.ipynb\n",
    "# ==========================================\n",
    "# [RDKit ä¸“å±] ç‰¹å¾å·¥ç¨‹ (å¥å£®æ€§å¢å¼ºç‰ˆ)\n",
    "# æ ¸å¿ƒæ”¹è¿›:\n",
    "# 1. ç»“æ„èšåˆ: å¯¹é‡å¤ SMILES çš„ Target å–å‡å€¼ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¸¢å¼ƒ\n",
    "# 2. å¥å£®åˆ‡åˆ†: å¢åŠ åˆ†ç®±å¤±è´¥çš„å…œåº•é€»è¾‘ï¼Œé˜²æ­¢æŠ¥é”™\n",
    "# 3. ç´¢å¼•ä¿æŠ¤: Imputer å¤„ç†åå¼ºåˆ¶è¿˜åŸ DataFrame æ ¼å¼ï¼Œç¡®ä¿ç´¢å¼•å¯¹é½\n",
    "# 4. è„æ•°æ®é˜²å¾¡: å¢åŠ å¯¹éå­—ç¬¦ä¸² SMILES çš„è¿‡æ»¤\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "\n",
    "# 1. ç¯å¢ƒå‡†å¤‡\n",
    "if 'notebooks' in os.getcwd(): os.chdir('..')\n",
    "print(f\"ğŸš€ å¯åŠ¨ RDKit ç‰¹å¾å·¥ç¨‹ (Robust Mode) | ç›®å½•: {os.getcwd()}\")\n",
    "\n",
    "# å…ƒæ•°æ®è®°å½•\n",
    "metadata = {\n",
    "    \"step\": \"01_feature_engineering\",\n",
    "    \"metrics\": {}\n",
    "}\n",
    "\n",
    "# ========================================================\n",
    "# [Step 1] è¯»å–æ•°æ® & è¯†åˆ«åˆ—å\n",
    "# ========================================================\n",
    "print(\"\\n>>> [Step 1] Loading Raw Data...\")\n",
    "raw_dir = 'data/raw'\n",
    "if not os.path.exists(raw_dir): raise FileNotFoundError(f\"âŒ æ‰¾ä¸åˆ° {raw_dir}\")\n",
    "\n",
    "# æŒ‰æ–‡ä»¶åæ’åºè¯»å–ï¼Œä¿è¯é¡ºåºä¸€è‡´\n",
    "csv_files = sorted([f for f in os.listdir(raw_dir) if f.lower().endswith('.csv')])\n",
    "if not csv_files: raise FileNotFoundError(f\"âŒ {raw_dir} ä¸ºç©º\")\n",
    "\n",
    "file_path = os.path.join(raw_dir, csv_files[0])\n",
    "df_raw = pd.read_csv(file_path)\n",
    "print(f\"   è¯»å–æ–‡ä»¶: {csv_files[0]} | åŸå§‹è¡Œæ•°: {len(df_raw)}\")\n",
    "\n",
    "# å»é™¤åˆ—åç©ºæ ¼\n",
    "df_raw.columns = df_raw.columns.str.strip()\n",
    "\n",
    "# 1. é”å®š Target\n",
    "target_col = None\n",
    "for c in ['log(1o2)', 'Target', 'Activity', 'Y', 'pIC50']:\n",
    "    if c in df_raw.columns: target_col = c; break\n",
    "if not target_col:\n",
    "    for c in df_raw.columns:\n",
    "        if 'log' in c.lower() or 'target' in c.lower(): target_col = c; break\n",
    "\n",
    "if not target_col: raise ValueError(f\"âŒ æ— æ³•è¯†åˆ« Target åˆ—! ç°æœ‰åˆ—: {list(df_raw.columns)}\")\n",
    "\n",
    "# 2. é”å®š SMILES\n",
    "smiles_col = 'SMILES' if 'SMILES' in df_raw.columns else df_raw.columns[0]\n",
    "print(f\"   Target: '{target_col}' | SMILES: '{smiles_col}'\")\n",
    "\n",
    "# 3. åˆæ­¥æ¸…æ´—: å»é™¤ Target ç¼ºå¤±è¡Œ\n",
    "init_len = len(df_raw)\n",
    "df_raw = df_raw.dropna(subset=[target_col])\n",
    "# å¼ºåˆ¶è½¬ä¸ºæ•°å€¼ï¼Œæ— æ³•è½¬æ¢çš„å˜ä¸º NaN ç„¶ååˆ æ‰\n",
    "df_raw[target_col] = pd.to_numeric(df_raw[target_col], errors='coerce')\n",
    "df_raw = df_raw.dropna(subset=[target_col])\n",
    "\n",
    "if len(df_raw) < init_len:\n",
    "    print(f\"   âš ï¸ å‰”é™¤äº† {init_len - len(df_raw)} è¡Œæ— æ•ˆ Target æ•°æ®\")\n",
    "\n",
    "# ========================================================\n",
    "# [Step 2] RDKit å¤„ç† (Canonicalize + Aggregation)\n",
    "# ========================================================\n",
    "print(\"\\n>>> [Step 2] RDKit Processing (Canonicalization & Aggregation)...\")\n",
    "\n",
    "desc_names = [x[0] for x in Descriptors._descList]\n",
    "calc = MoleculeDescriptors.MolecularDescriptorCalculator(desc_names)\n",
    "\n",
    "valid_rows = []\n",
    "invalid_smiles = 0\n",
    "\n",
    "for idx, row in df_raw.iterrows():\n",
    "    smi = row[smiles_col]\n",
    "    val = row[target_col]\n",
    "\n",
    "    # é˜²å¾¡: ç¡®ä¿ smi æ˜¯å­—ç¬¦ä¸²\n",
    "    if not isinstance(smi, str):\n",
    "        invalid_smiles += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            # æ ‡å‡†åŒ–\n",
    "            canon_smi = Chem.MolToSmiles(mol, isomericSmiles=True)\n",
    "            # è®¡ç®—ç‰¹å¾\n",
    "            feats = list(calc.CalcDescriptors(mol))\n",
    "\n",
    "            valid_rows.append({\n",
    "                'SMILES_Canonical': canon_smi,\n",
    "                'Target_Raw': val,\n",
    "                'Features': feats\n",
    "            })\n",
    "        else:\n",
    "            invalid_smiles += 1\n",
    "    except:\n",
    "        invalid_smiles += 1\n",
    "        continue\n",
    "\n",
    "print(f\"   - åŸå§‹æœ‰æ•ˆåˆ†å­æ•°: {len(valid_rows)}\")\n",
    "print(f\"   - æ— æ•ˆ/è§£æå¤±è´¥ SMILES: {invalid_smiles}\")\n",
    "\n",
    "# â˜…â˜…â˜… æ ¸å¿ƒæ”¹è¿›: ç»“æ„èšåˆ (Aggregation) â˜…â˜…â˜…\n",
    "# å¦‚æœåŒä¸€ä¸ª SMILES å‡ºç°å¤šæ¬¡ï¼Œå– Target çš„å¹³å‡å€¼\n",
    "df_temp = pd.DataFrame(valid_rows)\n",
    "# æŒ‰ SMILES_Canonical åˆ†ç»„ï¼ŒTarget å–å‡å€¼ï¼Œç‰¹å¾å–ç¬¬ä¸€æ¡(ç†è®ºä¸Šç‰¹å¾åº”è¯¥æ˜¯ä¸€æ ·çš„)\n",
    "# æ³¨æ„ï¼šç‰¹å¾åˆ—æ˜¯ listï¼Œä¸èƒ½ç›´æ¥ groupbyï¼Œæˆ‘ä»¬å…ˆèšåˆ Target\n",
    "df_agg = df_temp.groupby('SMILES_Canonical').agg({\n",
    "    'Target_Raw': 'mean',\n",
    "    'Features': 'first' # ç‰¹å¾å¯¹äºåŒä¸€ç»“æ„æ˜¯å›ºå®šçš„\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"   - èšåˆåå”¯ä¸€ç»“æ„æ•°: {len(df_agg)} (åˆå¹¶äº† {len(valid_rows) - len(df_agg)} ä¸ªé‡å¤ç»“æ„)\")\n",
    "\n",
    "# å±•å¼€ç‰¹å¾åˆ—\n",
    "# å°† Features (list) æ‹†åˆ†æˆå¤šåˆ—\n",
    "feat_data = pd.DataFrame(df_agg['Features'].tolist(), columns=desc_names)\n",
    "# åˆå¹¶å›å»\n",
    "df_features = pd.concat([\n",
    "    df_agg[['Target_Raw', 'SMILES_Canonical']],\n",
    "    feat_data\n",
    "], axis=1)\n",
    "\n",
    "# é‡å‘½åä»¥ç¬¦åˆåç»­æµç¨‹\n",
    "df_features.rename(columns={\n",
    "    'Target_Raw': 'Target_Log1o2',\n",
    "    'SMILES_Canonical': 'SMILES_Meta'\n",
    "}, inplace=True)\n",
    "\n",
    "# åˆæ­¥æ¸…æ´—æ— ç©·å¤§\n",
    "df_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# ========================================================\n",
    "# [Step 3] åŠ¨æ€åˆ†å±‚åˆ‡åˆ† (Robust Stratified Split)\n",
    "# ========================================================\n",
    "print(\"\\n>>> [Step 3] Stratified Splitting...\")\n",
    "\n",
    "# è¯»å–ç§å­\n",
    "seed_file = \"current_seed_config.txt\"\n",
    "if os.path.exists(seed_file):\n",
    "    try: CURRENT_SEED = int(open(seed_file).read().strip())\n",
    "    except: CURRENT_SEED = 288\n",
    "else: CURRENT_SEED = 288\n",
    "print(f\"   Seed: {CURRENT_SEED}\")\n",
    "\n",
    "X = df_features.drop(columns=['Target_Log1o2', 'SMILES_Meta'])\n",
    "y = df_features['Target_Log1o2']\n",
    "meta = df_features['SMILES_Meta']\n",
    "\n",
    "# â˜… æ”¹è¿›: å¥å£®çš„åˆ†ç®±é€»è¾‘ â˜…\n",
    "n_samples = len(y)\n",
    "# è‡³å°‘è¦æœ‰ 5 ä¸ªæ ·æœ¬æ‰èƒ½åˆ† 2 ç®±ï¼Œå¦åˆ™åˆ†ç®±æ— æ„ä¹‰\n",
    "num_bins = min(10, int(n_samples / 5))\n",
    "\n",
    "if num_bins < 2:\n",
    "    print(\"   âš ï¸ æ ·æœ¬è¿‡å°‘ï¼Œæ— æ³•åˆ†å±‚ï¼Œå›é€€åˆ°éšæœºåˆ‡åˆ†\")\n",
    "    stratify_col = None\n",
    "else:\n",
    "    try:\n",
    "        # qcut å¯èƒ½ä¼šå› ä¸ºæ•°æ®é‡å¤å¯¼è‡´ bin è¾¹ç•Œé‡å ï¼Œä½¿ç”¨ duplicates='drop'\n",
    "        stratify_col = pd.qcut(y, q=num_bins, labels=False, duplicates='drop')\n",
    "        # å†æ¬¡æ£€æŸ¥ï¼šå¦‚æœ drop ååªå‰© 1 ä¸ª binï¼Œä¹Ÿæ— æ³•åˆ†å±‚\n",
    "        if stratify_col.nunique() < 2:\n",
    "            print(\"   âš ï¸ Target åˆ†å¸ƒè¿‡äºé›†ä¸­ï¼Œæ— æ³•æœ‰æ•ˆåˆ†å±‚ï¼Œå›é€€åˆ°éšæœºåˆ‡åˆ†\")\n",
    "            stratify_col = None\n",
    "    except:\n",
    "        stratify_col = None\n",
    "\n",
    "# ç»„åˆä»¥ä¾¿åˆ‡åˆ†\n",
    "data_full = pd.concat([y, meta, X], axis=1)\n",
    "\n",
    "# ç¬¬ä¸€åˆ€ (Test 15%)\n",
    "try:\n",
    "    train_val, test = train_test_split(data_full, test_size=0.15, random_state=CURRENT_SEED, stratify=stratify_col)\n",
    "except ValueError:\n",
    "    # å¦‚æœæŸä¸ªç±»åˆ«çš„æ ·æœ¬å¤ªå°‘(æ¯”å¦‚åªæœ‰1ä¸ª)ï¼Œstratify ä¼šæŠ¥é”™\n",
    "    print(\"   âš ï¸ åˆ†å±‚åˆ‡åˆ†å¤±è´¥ (æŸç±»åˆ«æ ·æœ¬ä¸è¶³)ï¼Œå›é€€åˆ°éšæœºåˆ‡åˆ†\")\n",
    "    train_val, test = train_test_split(data_full, test_size=0.15, random_state=CURRENT_SEED)\n",
    "\n",
    "# é‡æ–°ä¸º Train+Val ç”Ÿæˆåˆ†å±‚ä¾æ®\n",
    "if stratify_col is not None:\n",
    "    y_tv = train_val['Target_Log1o2']\n",
    "    try:\n",
    "        num_bins_tv = min(10, int(len(y_tv) / 5))\n",
    "        if num_bins_tv >= 2:\n",
    "            stratify_tv = pd.qcut(y_tv, q=num_bins_tv, labels=False, duplicates='drop')\n",
    "        else: stratify_tv = None\n",
    "    except: stratify_tv = None\n",
    "else:\n",
    "    stratify_tv = None\n",
    "\n",
    "# ç¬¬äºŒåˆ€ (Val 15% of Total -> ~17.6% of TrainVal)\n",
    "try:\n",
    "    train, val = train_test_split(train_val, test_size=15/85, random_state=CURRENT_SEED, stratify=stratify_tv)\n",
    "except:\n",
    "    train, val = train_test_split(train_val, test_size=15/85, random_state=CURRENT_SEED)\n",
    "\n",
    "# æ‹†è§£å‡½æ•°\n",
    "def split_xy_meta(df):\n",
    "    return df.drop(columns=['Target_Log1o2', 'SMILES_Meta']), df['Target_Log1o2'], df['SMILES_Meta']\n",
    "\n",
    "X_train, y_train, s_train = split_xy_meta(train)\n",
    "X_val, y_val, s_val = split_xy_meta(val)\n",
    "X_test, y_test, s_test = split_xy_meta(test)\n",
    "\n",
    "print(f\"   Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# ========================================================\n",
    "# [Step 4] ä¸¥è°¨æ¸…æ´— (Fit on Train, Keep DataFrame)\n",
    "# ========================================================\n",
    "print(\"\\n>>> [Step 4] Rigorous Cleaning (Fit on Train)...\")\n",
    "\n",
    "# 1. Imputer (ä¿æŠ¤ Index)\n",
    "# ----------------------------------------------------\n",
    "print(\"   -> Imputing...\")\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "# Fit\n",
    "imputer.fit(X_train)\n",
    "\n",
    "# Transform å¹¶ç«‹åˆ»è¿˜åŸä¸º DataFrame (ä¿ç•™åˆ—åå’Œç´¢å¼•)\n",
    "def impute_df(X_df, imp):\n",
    "    data_imputed = imp.transform(X_df)\n",
    "    return pd.DataFrame(data_imputed, columns=X_df.columns, index=X_df.index)\n",
    "\n",
    "X_train = impute_df(X_train, imputer)\n",
    "X_val   = impute_df(X_val, imputer)\n",
    "X_test  = impute_df(X_test, imputer)\n",
    "\n",
    "# 2. å¸¸é‡è¿‡æ»¤ (ç›´æ¥åˆ¤æ–­ nunique)\n",
    "# ----------------------------------------------------\n",
    "# æ¯” VarianceThreshold æ›´ç›´è§‚ï¼Œæµ®ç‚¹æ•°ä¸æ•æ„Ÿ\n",
    "print(\"   -> Removing Constants...\")\n",
    "# æ‰¾å‡º Train ä¸­åªæœ‰ä¸€ä¸ªå€¼çš„åˆ—\n",
    "const_cols = [c for c in X_train.columns if X_train[c].nunique() <= 1]\n",
    "\n",
    "if const_cols:\n",
    "    X_train.drop(columns=const_cols, inplace=True)\n",
    "    X_val.drop(columns=const_cols, inplace=True)\n",
    "    X_test.drop(columns=const_cols, inplace=True)\n",
    "    print(f\"      Deleted {len(const_cols)} constant features\")\n",
    "\n",
    "# 3. é«˜ç›¸å…³è¿‡æ»¤ (Pandas Corr)\n",
    "# ----------------------------------------------------\n",
    "print(\"   -> Removing High Correlation (>0.90)...\")\n",
    "# åªåœ¨ Train ä¸Šè®¡ç®—\n",
    "corr_matrix = X_train.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "\n",
    "if to_drop:\n",
    "    X_train.drop(columns=to_drop, inplace=True)\n",
    "    X_val.drop(columns=to_drop, inplace=True)\n",
    "    X_test.drop(columns=to_drop, inplace=True)\n",
    "    print(f\"      Deleted {len(to_drop)} correlated features\")\n",
    "\n",
    "print(f\"   âœ… æœ€ç»ˆç‰¹å¾æ•°: {X_train.shape[1]}\")\n",
    "\n",
    "# ========================================================\n",
    "# [Step 5] ä¿å­˜\n",
    "# ========================================================\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('results/logs', exist_ok=True)\n",
    "\n",
    "# ä¿å­˜æ•°æ®\n",
    "def save_data(X, y, s, name):\n",
    "    out = X.copy()\n",
    "    out['Target_Log1o2'] = y\n",
    "    out['SMILES_Meta'] = s\n",
    "    out.to_csv(f'data/processed/{name}', index=False)\n",
    "\n",
    "save_data(X_train, y_train, s_train, 'train.csv')\n",
    "save_data(X_val, y_val, s_val, 'val.csv')\n",
    "save_data(X_test, y_test, s_test, 'test.csv')\n",
    "\n",
    "# ä¿å­˜ä¸€ä»½å…ƒæ•°æ®æ—¥å¿— (è®°å½•åˆ é™¤äº†å“ªäº›åˆ—)\n",
    "log_data = {\n",
    "    \"final_features\": list(X_train.columns),\n",
    "    \"dropped_constant\": const_cols,\n",
    "    \"dropped_correlation\": to_drop\n",
    "}\n",
    "with open('results/logs/feature_log.json', 'w') as f:\n",
    "    json.dump(log_data, f, indent=4)\n",
    "\n",
    "print(f\"\\nğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³ data/processed/\")\n",
    "print(f\"ğŸ“ ç‰¹å¾æ—¥å¿—å·²ä¿å­˜è‡³ results/logs/feature_log.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
