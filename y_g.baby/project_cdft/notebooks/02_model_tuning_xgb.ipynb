{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-24T13:56:12.657198Z",
     "iopub.status.busy": "2026-01-24T13:56:12.656199Z",
     "iopub.status.idle": "2026-01-24T13:56:24.845318Z",
     "shell.execute_reply": "2026-01-24T13:56:24.844815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ [02] å¯åŠ¨ CDFT è°ƒå‚ (Focus Mode) | ç›®å½•: D:\\HO\\my_smiles_project\\y_g.baby\\project_cdft\n",
      "âœ… è¯»å–è®­ç»ƒé›†: (105, 25)\n",
      ">>> [Step 2] Tuning XGBoost (5-Fold CV)...\n",
      "Fitting 25 folds for each of 60 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ† æœ€ä½³å‚æ•°: {'xgb__colsample_bytree': 1.0, 'xgb__learning_rate': np.float64(0.021371153508685993), 'xgb__max_depth': 2, 'xgb__n_estimators': 86, 'xgb__reg_alpha': 0, 'xgb__reg_lambda': np.float64(0.4103449926306416), 'xgb__subsample': np.float64(0.8955619904286328)}\n",
      "   æœ€ä½³ CV RMSE: 1.2722\n",
      "ğŸ’¾ å‚æ•°å·²ä¿å­˜è‡³ results/models/best_params_xgb.json\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# File: project_cdft/notebooks/02_model_tuning_xgb.ipynb\n",
    "# ==========================================\n",
    "# [CDFT ä¸“å±] æ¨¡å‹è°ƒä¼˜ (ä¸“æ³¨ç‰ˆ)\n",
    "# ä¼˜åŒ–ç­–ç•¥:\n",
    "# 1. å‡å°‘æœç´¢ç»´åº¦: å›ºå®š colsample_bytree=1.0 (ç‰¹å¾å°‘ï¼Œä¸éšæœºä¸¢å¼ƒ)\n",
    "# 2. å‡å°‘è¡¥å¿æ•ˆåº”: åªæœ L2 æ­£åˆ™ (lambda)ï¼Œå›ºå®š L1 (alpha)=0\n",
    "# 3. èšç„¦æ ‘ç»“æ„: ä»…åœ¨ æ·±åº¦(Depth) å’Œ å­¦ä¹ ç‡(Eta) ä¸Šåšæ–‡ç« \n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "\n",
    "# 1. ç¯å¢ƒå‡†å¤‡\n",
    "if 'notebooks' in os.getcwd(): os.chdir('..')\n",
    "print(f\"ğŸš€ [02] å¯åŠ¨ CDFT è°ƒå‚ (Focus Mode) | ç›®å½•: {os.getcwd()}\")\n",
    "\n",
    "# 2. è¯»å–æ•°æ® (åªè¯» Train)\n",
    "try:\n",
    "    train = pd.read_csv('data/processed/train.csv')\n",
    "    print(f\"âœ… è¯»å–è®­ç»ƒé›†: {train.shape}\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"âŒ æ‰¾ä¸åˆ° data/processed/train.csvï¼Œè¯·å…ˆè¿è¡Œ 01 å·æ–‡ä»¶\")\n",
    "\n",
    "target_col = 'Target_Log1o2' if 'Target_Log1o2' in train.columns else 'Target'\n",
    "y_train = train[target_col]\n",
    "X_train = train.drop(columns=[target_col, 'SMILES_Meta'], errors='ignore').select_dtypes(include=[np.number])\n",
    "\n",
    "# 3. æ„å»ºç®¡é“ (CDFT æ•°å€¼é€šå¸¸éœ€è¦æ ‡å‡†åŒ–)\n",
    "# æ³¨æ„: è¿™é‡Œå»æ‰äº† PowerTransformerï¼Œå› ä¸º CDFT ç‰©ç†é‡é€šå¸¸ä¸éœ€è¦å¼ºè¡Œæ­£æ€åŒ–ï¼ŒStandardScaler è¶³å¤Ÿ\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', XGBRegressor(objective='reg:squarederror', n_jobs=-1, random_state=42))\n",
    "])\n",
    "\n",
    "# 4. å‚æ•°ç©ºé—´ (ç˜¦èº«ç‰ˆ)\n",
    "# å»æ‰äº† colsample_bytree (å›ºå®š1.0)\n",
    "# å»æ‰äº† reg_alpha (å›ºå®š0)\n",
    "param_dist = {\n",
    "    # æ ¸å¿ƒå¤æ‚åº¦æ§åˆ¶\n",
    "    'xgb__max_depth': [2, 3, 4],  # æç®€æ ‘æ·±ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "\n",
    "    # å­¦ä¹ ç‡ä¸æ ‘æ•°é‡ (è¿™å¯¹å†¤å®¶è¿˜æ˜¯è¦æœä¸€ä¸‹çš„)\n",
    "    'xgb__learning_rate': loguniform(0.01, 0.15), # ç¨å¾®é™åˆ¶ä¸Šé™ï¼Œä¸è®©å®ƒå­¦å¤ªå¿«\n",
    "    'xgb__n_estimators': randint(80, 300),        # ä¹Ÿä¸éœ€è¦ä¸Šåƒæ£µæ ‘\n",
    "\n",
    "    # ç¨³å®šæ€§æ§åˆ¶\n",
    "    'xgb__subsample': uniform(0.6, 0.3),          # 0.6 ~ 0.9ï¼Œä¿è¯æ ·æœ¬è¦†ç›–åº¦\n",
    "\n",
    "    # æ­£åˆ™åŒ– (åªæœ L2)\n",
    "    'xgb__reg_lambda': loguniform(0.1, 5.0),      # L2 æ­£åˆ™\n",
    "    'xgb__reg_alpha': [0],                        # L1 å›ºå®šä¸º 0ï¼Œå‡å°‘ç»´åº¦å¹²æ‰°\n",
    "\n",
    "    # ç‰¹å¾é‡‡æ · (å›ºå®š)\n",
    "    'xgb__colsample_bytree': [1.0]                # CDFT ç‰¹å¾éƒ½æ˜¯ç²¾åï¼Œå…¨éƒ½è¦ï¼\n",
    "}\n",
    "\n",
    "# 5. æ‰§è¡Œæœç´¢\n",
    "print(\">>> [Step 2] Tuning XGBoost (5-Fold CV)...\")\n",
    "# è¿™é‡Œçš„ n_iter å¯ä»¥ç¨å¾®è®¾å°ä¸€ç‚¹ï¼Œå› ä¸ºæœç´¢ç©ºé—´å˜å°äº†\n",
    "rkf = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_dist,\n",
    "    n_iter=60,          # 60æ¬¡è¶³çŸ£ï¼Œç©ºé—´ä¸å¤§\n",
    "    cv=rkf,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# 6. ä¿å­˜ç»“æœ\n",
    "best_params = search.best_params_\n",
    "best_rmse = np.sqrt(-search.best_score_)\n",
    "\n",
    "print(f\"\\nğŸ† æœ€ä½³å‚æ•°: {best_params}\")\n",
    "print(f\"   æœ€ä½³ CV RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "os.makedirs('results/models', exist_ok=True)\n",
    "# æ³¨æ„ï¼šè¿™é‡Œå­˜çš„æ—¶å€™è¦æŠŠ pipeline çš„å‰ç¼€ 'xgb__' å»æ‰ï¼Œæ–¹ä¾¿ 03 å·æ–‡ä»¶ç›´æ¥ç”¨\n",
    "clean_params = {k.replace('xgb__', ''): v for k, v in best_params.items()}\n",
    "\n",
    "with open('results/models/best_params_xgb.json', 'w') as f:\n",
    "    json.dump(clean_params, f, indent=4)\n",
    "\n",
    "print(f\"ğŸ’¾ å‚æ•°å·²ä¿å­˜è‡³ results/models/best_params_xgb.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
