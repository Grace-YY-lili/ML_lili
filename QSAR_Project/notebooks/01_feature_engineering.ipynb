{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-27T02:35:41.359428Z",
     "iopub.status.busy": "2026-01-27T02:35:41.359428Z",
     "iopub.status.idle": "2026-01-27T02:35:43.863726Z",
     "shell.execute_reply": "2026-01-27T02:35:43.863726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [Step 1] Reading Data & Generating Features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Raw Features Shape: (151, 240)\n",
      "   -> [Raw_Generated] Remaining: 240 (Dropped: 0)\n",
      "\n",
      ">>> [Step 2] Dynamic Splitting (Train/Val/Test)...\n",
      "ğŸ“¨ æ”¶åˆ°å¤–éƒ¨æŒ‡ä»¤: ä½¿ç”¨éšæœºç§å­ Seed = 288\n",
      "\n",
      ">>> [Step 3] Processing Pipeline (Fit on Train)...\n",
      "   Processing Imputation (Median)...\n",
      "   Processing Quasi-Constant Filter (threshold=0.99)...\n",
      "   -> [Variance_Filter] Remaining: 188 (Dropped: 52)\n",
      "   Processing Correlation Filter (threshold=0.90)...\n",
      "   -> [Correlation_Filter] Remaining: 126 (Dropped: 62)\n",
      "   Processing ElasticNet Selection...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> [ElasticNet_Selection] Remaining: 21 (Dropped: 105)\n",
      "\n",
      ">>> [Step 4] Saving Results & Logs...\n",
      "âœ… Success! ElasticNet kept 21 robust features.\n",
      "   Alpha: 0.231579, L1_Ratio: 0.5\n",
      "   Logs saved to results/logs/\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# File: 01_feature_engineering.ipynb (Partner Edition)\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "\n",
    "# 1. ä¿®æ­£è·¯å¾„\n",
    "if 'notebooks' in os.getcwd(): os.chdir('..')\n",
    "\n",
    "# ========================================================\n",
    "# è¾…åŠ©å‡½æ•°ï¼šæ—¥å¿—ç³»ç»Ÿ\n",
    "# ========================================================\n",
    "feature_log = {}\n",
    "deleted_features = {}\n",
    "\n",
    "def log_step(step_name, current_cols, dropped_cols=None):\n",
    "    feature_log[step_name] = len(current_cols)\n",
    "    if dropped_cols is not None and len(dropped_cols) > 0:\n",
    "        deleted_features[step_name] = list(dropped_cols)\n",
    "    print(f\"   -> [{step_name}] Remaining: {len(current_cols)} (Dropped: {len(dropped_cols) if dropped_cols is not None else 0})\")\n",
    "\n",
    "# ========================================================\n",
    "# é˜¶æ®µ 1: ç”ŸæˆåŸå§‹ç‰¹å¾\n",
    "# ========================================================\n",
    "print(\">>> [Step 1] Reading Data & Generating Features...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('data/raw/cdft_qsar.CSV')\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"âŒ æ‰¾ä¸åˆ° data/raw/cdft_qsar.CSV\")\n",
    "\n",
    "target_col = 'log(1o2)'\n",
    "cols_to_drop = [target_col, 'ID', 'dup_count']\n",
    "X_raw_cdft = df.drop(columns=[c for c in cols_to_drop if c in df.columns]).select_dtypes(include=[np.number])\n",
    "y_all = df[target_col]\n",
    "smiles_all = df['SMILES']\n",
    "\n",
    "# RDKit ç”Ÿæˆ\n",
    "desc_names = [x[0] for x in Descriptors._descList]\n",
    "calc = MoleculeDescriptors.MolecularDescriptorCalculator(desc_names)\n",
    "mols = [Chem.MolFromSmiles(s) for s in smiles_all]\n",
    "valid_idx = [i for i, m in enumerate(mols) if m is not None]\n",
    "X_raw_rdkit = pd.DataFrame([calc.CalcDescriptors(mols[i]) for i in valid_idx], columns=desc_names)\n",
    "\n",
    "# åˆå¹¶ä¸æ¸…æ´— Inf\n",
    "X_full = pd.concat([X_raw_cdft.iloc[valid_idx].reset_index(drop=True),\n",
    "                    X_raw_rdkit.reset_index(drop=True)], axis=1)\n",
    "y_full = y_all.iloc[valid_idx].reset_index(drop=True)\n",
    "smiles_full = smiles_all.iloc[valid_idx].reset_index(drop=True)\n",
    "X_full.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "print(f\"ğŸ“Š Raw Features Shape: {X_full.shape}\")\n",
    "log_step(\"Raw_Generated\", X_full.columns)\n",
    "\n",
    "# ========================================================\n",
    "# é˜¶æ®µ 2: åŠ¨æ€åˆ‡åˆ† (Dynamic Splitting)\n",
    "# ========================================================\n",
    "print(\"\\n>>> [Step 2] Dynamic Splitting (Train/Val/Test)...\")\n",
    "\n",
    "data_w_meta = X_full.copy()\n",
    "data_w_meta['Target_Log1o2'] = y_full\n",
    "data_w_meta['SMILES'] = smiles_full\n",
    "\n",
    "# â˜…â˜…â˜… å°è¯•è¯»å–å¤–éƒ¨ç§å­ â˜…â˜…â˜…\n",
    "seed_file = \"current_seed_config.txt\"\n",
    "if os.path.exists(seed_file):\n",
    "    with open(seed_file, \"r\") as f:\n",
    "        try:\n",
    "            CURRENT_SEED = int(f.read().strip())\n",
    "            print(f\"ğŸ“¨ æ”¶åˆ°å¤–éƒ¨æŒ‡ä»¤: ä½¿ç”¨éšæœºç§å­ Seed = {CURRENT_SEED}\")\n",
    "        except:\n",
    "            CURRENT_SEED = 888 # è¯»å–å¤±è´¥æ—¶çš„å…œåº•\n",
    "            print(f\"âš ï¸ è¯»å–æŒ‡ä»¤å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç§å­ Seed = {CURRENT_SEED}\")\n",
    "else:\n",
    "    CURRENT_SEED = 888 # æ²¡çº¸æ¡æ—¶çš„é»˜è®¤å€¼\n",
    "    print(f\"âš ï¸ æœªæ”¶åˆ°æŒ‡ä»¤ï¼Œä½¿ç”¨é»˜è®¤ç§å­ Seed = {CURRENT_SEED}\")\n",
    "\n",
    "# ä½¿ç”¨è¯»å–åˆ°çš„ CURRENT_SEED è¿›è¡Œåˆ‡åˆ† (70/15/15)\n",
    "train_val_set, test_set = train_test_split(data_w_meta, test_size=0.15, random_state=CURRENT_SEED)\n",
    "train_set, val_set = train_test_split(train_val_set, test_size=15/85, random_state=CURRENT_SEED)\n",
    "\n",
    "def split_features(df):\n",
    "    y = df['Target_Log1o2']\n",
    "    meta = df['SMILES']\n",
    "    X = df.drop(columns=['Target_Log1o2', 'SMILES'])\n",
    "    return X, y, meta\n",
    "\n",
    "X_train, y_train, s_train = split_features(train_set)\n",
    "X_val,   y_val,   s_val   = split_features(val_set)\n",
    "X_test,  y_test,  s_test  = split_features(test_set)\n",
    "\n",
    "# ========================================================\n",
    "# é˜¶æ®µ 3: ä¸¥æ ¼é˜²æ³„æ¼æ¸…æ´—æµç¨‹ (Fit on Train ONLY)\n",
    "# ========================================================\n",
    "print(\"\\n>>> [Step 3] Processing Pipeline (Fit on Train)...\")\n",
    "\n",
    "# 1. Imputer (Median)\n",
    "print(\"   Processing Imputation (Median)...\")\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(X_train)\n",
    "cols = X_train.columns\n",
    "X_train_imp = pd.DataFrame(imputer.transform(X_train), columns=cols)\n",
    "X_val_imp   = pd.DataFrame(imputer.transform(X_val),   columns=cols)\n",
    "X_test_imp  = pd.DataFrame(imputer.transform(X_test),  columns=cols)\n",
    "\n",
    "# 2. Quasi-Constant Filter\n",
    "print(\"   Processing Quasi-Constant Filter (threshold=0.99)...\")\n",
    "def get_quasi_constant_cols(df, threshold=0.99):\n",
    "    to_drop = []\n",
    "    for col in df.columns:\n",
    "        if df[col].value_counts(normalize=True).iloc[0] > threshold:\n",
    "            to_drop.append(col)\n",
    "    return to_drop\n",
    "\n",
    "vars_to_drop = get_quasi_constant_cols(X_train_imp)\n",
    "X_train_var = X_train_imp.drop(columns=vars_to_drop)\n",
    "X_val_var   = X_val_imp.drop(columns=vars_to_drop)\n",
    "X_test_var  = X_test_imp.drop(columns=vars_to_drop)\n",
    "log_step(\"Variance_Filter\", X_train_var.columns, dropped_cols=vars_to_drop)\n",
    "\n",
    "# 3. Correlation Filter (ä½ çš„å»ºè®®1ï¼šæ”¹è¿›ç‰ˆ)\n",
    "# æˆ‘ä»¬ä½¿ç”¨ np.number åŒ…å«æ•´æ•°ï¼ˆå¦‚ç¯æ•°ï¼‰ï¼Œé˜²æ­¢è®¡æ•°å‹ç‰¹å¾å…±çº¿æ€§\n",
    "print(\"   Processing Correlation Filter (threshold=0.90)...\")\n",
    "numeric_cols = X_train_var.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = X_train_var[numeric_cols].corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "corr_to_drop = [col for col in upper.columns if any(upper[col] > 0.90)]\n",
    "\n",
    "X_train_corr = X_train_var.drop(columns=corr_to_drop)\n",
    "X_val_corr   = X_val_var.drop(columns=corr_to_drop)\n",
    "X_test_corr  = X_test_var.drop(columns=corr_to_drop)\n",
    "log_step(\"Correlation_Filter\", X_train_corr.columns, dropped_cols=corr_to_drop)\n",
    "\n",
    "# 4. ElasticNet Selection\n",
    "print(\"   Processing ElasticNet Selection...\")\n",
    "scaler_temp = StandardScaler()\n",
    "X_train_scaled = scaler_temp.fit_transform(X_train_corr)\n",
    "\n",
    "l1_ratios = [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1.0]\n",
    "elastic = ElasticNetCV(l1_ratio=l1_ratios, cv=5, random_state=42, n_jobs=-1, max_iter=20000)\n",
    "elastic.fit(X_train_scaled, y_train)\n",
    "\n",
    "model_sel = SelectFromModel(elastic, prefit=True)\n",
    "selected_mask = model_sel.get_support()\n",
    "selected_feats = X_train_corr.columns[selected_mask]\n",
    "dropped_elastic = X_train_corr.columns[~selected_mask]\n",
    "\n",
    "# ä½ çš„å»ºè®®2ï¼šæœ€å°ç‰¹å¾ä¿æŠ¤\n",
    "if len(selected_feats) < 5:\n",
    "    raise RuntimeError(\n",
    "        f\"âŒ CRITICAL ERROR: ElasticNet selected too few features ({len(selected_feats)}). \"\n",
    "        \"Check data stability or adjust l1_ratio grid.\"\n",
    "    )\n",
    "\n",
    "X_train_final = X_train_corr[selected_feats]\n",
    "X_val_final   = X_val_corr[selected_feats]\n",
    "X_test_final  = X_test_corr[selected_feats]\n",
    "\n",
    "log_step(\"ElasticNet_Selection\", X_train_final.columns, dropped_cols=dropped_elastic)\n",
    "\n",
    "# ========================================================\n",
    "# é˜¶æ®µ 4: ä¿å­˜ä¸æ—¥å¿— (ä½ çš„å»ºè®®3 & 4)\n",
    "# ========================================================\n",
    "print(\"\\n>>> [Step 4] Saving Results & Logs...\")\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('results/logs', exist_ok=True)\n",
    "\n",
    "# ä¿å­˜æ•°æ®\n",
    "def save_clean_csv(X, y, s, filename):\n",
    "    df_out = X.copy()\n",
    "    df_out['Target_Log1o2'] = y.values\n",
    "    df_out['SMILES'] = s.values\n",
    "    df_out.to_csv(filename, index=False)\n",
    "\n",
    "save_clean_csv(X_train_final, y_train, s_train, 'data/processed/train_data.csv')\n",
    "save_clean_csv(X_val_final,   y_val,   s_val,   'data/processed/val_data.csv')\n",
    "save_clean_csv(X_test_final,  y_test,  s_test,  'data/processed/test_data.csv')\n",
    "\n",
    "# ä½ çš„å»ºè®®3ï¼šä¿å­˜ç‰¹å¾æ•°å˜åŒ–æ—¥å¿—\n",
    "pd.DataFrame(\n",
    "    feature_log.items(),\n",
    "    columns=[\"Step\", \"Remaining_Features\"]\n",
    ").to_csv(\"results/logs/feature_count_by_step.csv\", index=False)\n",
    "\n",
    "# ä½ çš„å»ºè®®4ï¼šä¿å­˜ ElasticNet å‚æ•°\n",
    "with open(\"results/logs/elasticnet_params.txt\", \"w\") as f:\n",
    "    f.write(f\"alpha: {elastic.alpha_}\\n\")\n",
    "    f.write(f\"l1_ratio: {elastic.l1_ratio_}\\n\")\n",
    "    f.write(f\"n_features_in: {len(X_train_corr.columns)}\\n\")\n",
    "    f.write(f\"n_features_out: {len(selected_feats)}\\n\")\n",
    "\n",
    "# ä¿å­˜ç‰¹å¾æ¸…å•\n",
    "pd.DataFrame(selected_feats, columns=['Selected_Features']).to_csv('results/logs/final_feature_list.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Success! ElasticNet kept {len(selected_feats)} robust features.\")\n",
    "print(f\"   Alpha: {elastic.alpha_:.6f}, L1_Ratio: {elastic.l1_ratio_}\")\n",
    "print(\"   Logs saved to results/logs/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
